2020-07-14 17:37:17,192 - root - INFO - rank 1, begin data loader init (local rank 1)
2020-07-14 17:37:17,195 - root - INFO - ------------------ Configuration ------------------
2020-07-14 17:37:17,195 - root - INFO - Configuration file: /opt/DDP_UNet/config/UNet_transpose.yaml
2020-07-14 17:37:17,195 - root - INFO - Configuration name: default
2020-07-14 17:37:17,200 - root - INFO - weight_init ordereddict([('conv_init', 'normal'), ('conv_scale', 0.02), ('conv_bias', 0.0)])
2020-07-14 17:37:17,200 - root - INFO - lr 0.0001
2020-07-14 17:37:17,200 - root - INFO - data_path /data/DDP_trdat_tra.h5
2020-07-14 17:37:17,200 - root - INFO - transposed_input 1
2020-07-14 17:37:17,200 - root - INFO - rotate_input 1
2020-07-14 17:37:17,200 - root - INFO - ngpu 1
2020-07-14 17:37:17,200 - root - INFO - Nsamples 20
2020-07-14 17:37:17,200 - root - INFO - num_epochs 5
2020-07-14 17:37:17,200 - root - INFO - num_data_workers 2
2020-07-14 17:37:17,200 - root - INFO - LAMBDA_2 0.01
2020-07-14 17:37:17,200 - root - INFO - data_size 256
2020-07-14 17:37:17,200 - root - INFO - N_out_channels 5
2020-07-14 17:37:17,200 - root - INFO - batch_size 1
2020-07-14 17:37:17,200 - root - INFO - cpu_pipeline 0
2020-07-14 17:37:17,200 - root - INFO - ---------------------------------------------------
2020-07-14 17:37:17,200 - root - INFO - rank 0, begin data loader init (local rank 0)
Transposed Input
Transposed Input
Use Zero Copy ES
Enable Rotation
Use Zero Copy ES
Enable Rotation
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
2020-07-14 17:37:39,268 - root - INFO - rank 0, data loader initialized
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
2020-07-14 17:37:40,777 - root - INFO - rank 1, data loader initialized
2020-07-14 17:37:42,056 - root - INFO - DistributedDataParallel(
  (module): UNet(
    (conv_down1): Sequential(
      (0): Conv3d(4, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_down2): Sequential(
      (0): Conv3d(64, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_down3): Sequential(
      (0): Conv3d(128, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_down4): Sequential(
      (0): Conv3d(256, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_down5): Sequential(
      (0): Conv3d(512, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_down6): Sequential(
      (0): Conv3d(512, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_up6): Sequential(
      (0): ConvTranspose3d(512, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): ReLU(inplace=True)
    )
    (conv_up5): Sequential(
      (0): ConvTranspose3d(1024, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): ReLU(inplace=True)
    )
    (conv_up4): Sequential(
      (0): ConvTranspose3d(1024, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): ReLU(inplace=True)
    )
    (conv_up3): Sequential(
      (0): ConvTranspose3d(512, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): ReLU(inplace=True)
    )
    (conv_up2): Sequential(
      (0): ConvTranspose3d(256, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): ReLU(inplace=True)
    )
    (conv_last): ConvTranspose3d(128, 5, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
    (tanh): Tanh()
  )
)
2020-07-14 17:37:42,056 - root - INFO - Starting Training Loop...
2020-07-14 17:38:07,926 - root - INFO - Time taken for epoch 1 is 25.869626998901367 sec
2020-07-14 17:38:07,926 - root - INFO - total time / step = 1.2934755682945251, fw time / step = 0.07080941200256348, bw time / step = 1.1557345271110535, exposed io time / step = 0.06693162918090811, iters/s = 0.7731108530472834, logging time = 0.0
2020-07-14 17:38:32,774 - root - INFO - Time taken for epoch 2 is 24.847745895385742 sec
2020-07-14 17:38:32,774 - root - INFO - total time / step = 1.2423782348632812, fw time / step = 0.004855537414550781, bw time / step = 1.1672949314117431, exposed io time / step = 0.07022776603698744, iters/s = 0.8049078549014069, logging time = 0.0
2020-07-14 17:38:57,784 - root - INFO - Time taken for epoch 3 is 25.009088039398193 sec
2020-07-14 17:38:57,784 - root - INFO - total time / step = 1.250446617603302, fw time / step = 0.005206668376922607, bw time / step = 1.1749860644340515, exposed io time / step = 0.07025388479232797, iters/s = 0.7997142668246595, logging time = 0.0
2020-07-14 17:39:22,858 - root - INFO - Time taken for epoch 4 is 25.073728322982788 sec
2020-07-14 17:39:22,858 - root - INFO - total time / step = 1.253677451610565, fw time / step = 0.004670679569244385, bw time / step = 1.1787074089050293, exposed io time / step = 0.07029936313629137, iters/s = 0.7976533347675093, logging time = 0.0
2020-07-14 17:39:47,841 - root - INFO - Time taken for epoch 5 is 24.982710123062134 sec
2020-07-14 17:39:47,841 - root - INFO - total time / step = 1.2491269826889038, fw time / step = 0.004581809043884277, bw time / step = 1.174380612373352, exposed io time / step = 0.07016456127166748, iters/s = 0.8005591215773544, logging time = 0.0
2020-07-14 17:39:47,876 - root - INFO - DONE ---- rank 0
2020-07-14 17:39:47,877 - root - INFO - DONE ---- rank 1
