2020-06-23 03:10:52,470 - root - INFO - rank 1, begin data loader init (local rank 1)
2020-06-23 03:10:52,474 - root - INFO - ------------------ Configuration ------------------
2020-06-23 03:10:52,474 - root - INFO - Configuration file: /opt/DDP_UNet/config/UNet_transpose.yaml
2020-06-23 03:10:52,474 - root - INFO - Configuration name: default
2020-06-23 03:10:52,478 - root - INFO - weight_init ordereddict([('conv_init', 'normal'), ('conv_scale', 0.02), ('conv_bias', 0.0)])
2020-06-23 03:10:52,479 - root - INFO - lr 0.0001
2020-06-23 03:10:52,479 - root - INFO - data_path /data/DDP_trdat_tra.h5
2020-06-23 03:10:52,479 - root - INFO - transposed_input 1
2020-06-23 03:10:52,479 - root - INFO - rotate_input 1
2020-06-23 03:10:52,479 - root - INFO - ngpu 1
2020-06-23 03:10:52,479 - root - INFO - Nsamples 20
2020-06-23 03:10:52,479 - root - INFO - num_epochs 5
2020-06-23 03:10:52,479 - root - INFO - num_data_workers 2
2020-06-23 03:10:52,479 - root - INFO - LAMBDA_2 0.01
2020-06-23 03:10:52,479 - root - INFO - data_size 256
2020-06-23 03:10:52,479 - root - INFO - N_out_channels 5
2020-06-23 03:10:52,479 - root - INFO - batch_size 1
2020-06-23 03:10:52,479 - root - INFO - cpu_pipeline 0
2020-06-23 03:10:52,479 - root - INFO - ---------------------------------------------------
2020-06-23 03:10:52,479 - root - INFO - rank 0, begin data loader init (local rank 0)
Transposed Input
Transposed Input
Enable Rotation
Use GPU Pipeline
Enable Rotation
Use GPU Pipeline
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
2020-06-23 03:11:15,832 - root - INFO - rank 1, data loader initialized
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
2020-06-23 03:11:15,873 - root - INFO - rank 0, data loader initialized
2020-06-23 03:11:17,382 - root - INFO - DistributedDataParallel(
  (module): UNet(
    (conv_down1): Sequential(
      (0): Conv3d(4, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_down2): Sequential(
      (0): Conv3d(64, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_down3): Sequential(
      (0): Conv3d(128, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_down4): Sequential(
      (0): Conv3d(256, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_down5): Sequential(
      (0): Conv3d(512, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_down6): Sequential(
      (0): Conv3d(512, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (conv_up6): Sequential(
      (0): ConvTranspose3d(512, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): ReLU(inplace=True)
    )
    (conv_up5): Sequential(
      (0): ConvTranspose3d(1024, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): ReLU(inplace=True)
    )
    (conv_up4): Sequential(
      (0): ConvTranspose3d(1024, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): ReLU(inplace=True)
    )
    (conv_up3): Sequential(
      (0): ConvTranspose3d(512, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): ReLU(inplace=True)
    )
    (conv_up2): Sequential(
      (0): ConvTranspose3d(256, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
      (1): ReLU(inplace=True)
    )
    (conv_last): ConvTranspose3d(128, 5, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))
    (tanh): Tanh()
  )
)
2020-06-23 03:11:17,383 - root - INFO - Starting Training Loop...
2020-06-23 03:11:22,677 - root - INFO - Time taken for epoch 1 is 5.293477296829224 sec
2020-06-23 03:11:22,677 - root - INFO - total time / step = 0.26466808319091795, fw time / step = 0.0, bw time / step = 0.0, exposed io time / step = 0.26466808319091795, iters/s = 3.778317309528597, logging time = 0.0
2020-06-23 03:11:28,734 - root - INFO - Time taken for epoch 2 is 6.05654501914978 sec
2020-06-23 03:11:28,734 - root - INFO - total time / step = 0.3028185725212097, fw time / step = 0.0, bw time / step = 0.0, exposed io time / step = 0.3028185725212097, iters/s = 3.302307357419298, logging time = 0.0
2020-06-23 03:11:34,861 - root - INFO - Time taken for epoch 3 is 6.127024412155151 sec
2020-06-23 03:11:34,861 - root - INFO - total time / step = 0.3063423991203308, fw time / step = 0.0, bw time / step = 0.0, exposed io time / step = 0.3063423991203308, iters/s = 3.264321239474271, logging time = 0.0
2020-06-23 03:11:40,955 - root - INFO - Time taken for epoch 4 is 6.0940515995025635 sec
2020-06-23 03:11:40,956 - root - INFO - total time / step = 0.3046939492225647, fw time / step = 0.0, bw time / step = 0.0, exposed io time / step = 0.3046939492225647, iters/s = 3.281981813395141, logging time = 0.0
2020-06-23 03:11:47,060 - root - INFO - Time taken for epoch 5 is 6.10442590713501 sec
2020-06-23 03:11:47,061 - root - INFO - total time / step = 0.30521042346954347, fw time / step = 0.0, bw time / step = 0.0, exposed io time / step = 0.30521042346954347, iters/s = 3.276428074219387, logging time = 0.0
2020-06-23 03:11:47,365 - root - INFO - DONE ---- rank 0
2020-06-23 03:11:47,489 - root - INFO - DONE ---- rank 1
