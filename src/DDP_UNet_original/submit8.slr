#!/bin/bash -l
#SBATCH --nodes=1  --time=0:50:00  
#SBATCH -C gpu 
#SBATCH --account m1759
#SBATCH --gres=gpu:8
#SBATCH --exclusive
#SBATCH -c 80
#SBATCH -J multi8
#SBATCH -o %x-%j.out
#SBATCH --image=registry.services.nersc.gov/tkurth/pytorch-cosmo:new_scaler
#SBATCH --volume="/global/cscratch1/sd/tkurth/cosmo_data:/data;/dev/infiniband:/sys/class/infiniband_verbs"

# Start training
export HDF5_USE_FILE_LOCKING=FALSE
srun shifter python -m torch.distributed.launch --nproc_per_node=8 train.py --run_num=14 \
     --yaml_config=./config/UNet.yaml \
     --config=multi8

date

