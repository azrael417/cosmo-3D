Distributed computation initialized
Distributed computation initialized
Distributed computation initialized
2020-07-22 19:59:56,533 - root - INFO - rank 4, begin data loader init
2020-07-22 19:59:56,533 - root - INFO - rank 7, begin data loader init
2020-07-22 19:59:56,533 - root - INFO - rank 1, begin data loader init
Distributed computation initialized
Distributed computation initialized
2020-07-22 19:59:56,534 - root - INFO - rank 6, begin data loader init
2020-07-22 19:59:56,534 - root - INFO - rank 2, begin data loader init
Distributed computation initialized
2020-07-22 19:59:56,535 - root - INFO - rank 5, begin data loader init
Distributed computation initialized
2020-07-22 19:59:57,546 - root - INFO - rank 3, begin data loader init
Distributed computation initialized
2020-07-22 19:59:57,550 - root - INFO - ------------------ Configuration ------------------
2020-07-22 19:59:57,550 - root - INFO - Configuration file: /global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/config/UNet_transpose.yaml
2020-07-22 19:59:57,555 - root - INFO - Configuration name: multi8
2020-07-22 19:59:57,560 - root - INFO - ngpu 8
2020-07-22 19:59:57,560 - root - INFO - weight_init ordereddict([('conv_init', 'normal'), ('conv_scale', 0.02), ('conv_bias', 0.0)])
2020-07-22 19:59:57,560 - root - INFO - lr 0.0001
2020-07-22 19:59:57,560 - root - INFO - data_path /data/DDP_trdat_tra.h5
2020-07-22 19:59:57,561 - root - INFO - transposed_input 1
2020-07-22 19:59:57,561 - root - INFO - rotate_input 1
2020-07-22 19:59:57,561 - root - INFO - Nsamples 20
2020-07-22 19:59:57,561 - root - INFO - num_epochs 5
2020-07-22 19:59:57,561 - root - INFO - num_data_workers 2
2020-07-22 19:59:57,561 - root - INFO - LAMBDA_2 0.01
2020-07-22 19:59:57,561 - root - INFO - data_size 256
2020-07-22 19:59:57,561 - root - INFO - N_out_channels 5
2020-07-22 19:59:57,561 - root - INFO - batch_size 1
2020-07-22 19:59:57,561 - root - INFO - cpu_pipeline 0
2020-07-22 19:59:57,561 - root - INFO - ---------------------------------------------------
2020-07-22 19:59:57,561 - root - INFO - rank 0, begin data loader init
Transposed InputTransposed Input

Transposed InputTransposed Input

Transposed InputTransposed InputTransposed Input


Transposed Input
Use Zero Copy ES
Use Zero Copy ES
Use Zero Copy ES
Use Zero Copy ES
Enable Rotation
Enable Rotation
Enable Rotation
Enable Rotation
Use Zero Copy ES
Enable Rotation
Use Zero Copy ES
Enable Rotation
Use Zero Copy ES
Enable Rotation
Use Zero Copy ES
Enable Rotation
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py:124: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
Traceback (most recent call last):
Traceback (most recent call last):
  File "train.py", line 193, in <module>
Traceback (most recent call last):
  File "train.py", line 193, in <module>
Traceback (most recent call last):
  File "train.py", line 193, in <module>
  File "train.py", line 193, in <module>
    train(params, args, world_rank)
  File "train.py", line 42, in train
    train(params, args, world_rank)
  File "train.py", line 42, in train
    train(params, args, world_rank)
  File "train.py", line 42, in train
    train_data_loader = get_data_loader_distributed(params, world_rank)
      File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 23, in get_data_loader_distributed
train_data_loader = get_data_loader_distributed(params, world_rank)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 23, in get_data_loader_distributed
    train_data_loader = get_data_loader_distributed(params, world_rank)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 23, in get_data_loader_distributed
            train_loader = RandomCropDataLoader(params, num_workers=params.num_data_workers, device_id=device_id)train_loader = RandomCropDataLoader(params, num_workers=params.num_data_workers, device_id=device_id)train_loader = RandomCropDataLoader(params, num_workers=params.num_data_workers, device_id=device_id)


  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 306, in __init__
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 306, in __init__
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 306, in __init__
    self.iterator = DALIGenericIterator([self.pipe], ['inp', 'tar'], self.length, auto_reset = True)    
self.iterator = DALIGenericIterator([self.pipe], ['inp', 'tar'], self.length, auto_reset = True)
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 160, in __init__
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 160, in __init__
    self.iterator = DALIGenericIterator([self.pipe], ['inp', 'tar'], self.length, auto_reset = True)
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 160, in __init__
        self._first_batch = self.next()    
self._first_batch = self.next()self._first_batch = self.next()

  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py", line 273, in next
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py", line 273, in next
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py", line 273, in next
    return self.__next__()    
return self.__next__()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 174, in __next__
      File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 174, in __next__
return self.__next__()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 174, in __next__
    outputs.append(p.share_outputs())    
outputs.append(p.share_outputs())
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/pipeline.py", line 663, in share_outputs
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/pipeline.py", line 663, in share_outputs
    outputs.append(p.share_outputs())
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/pipeline.py", line 663, in share_outputs
    train(params, args, world_rank)
  File "train.py", line 42, in train
        return self._pipe.ShareOutputs()train_data_loader = get_data_loader_distributed(params, world_rank)

  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 23, in get_data_loader_distributed
RuntimeError: Critical error in pipeline:
Error when executing GPU operator Rotate, instance name: "__Rotate_5", encountered:
CUDA allocation failed
Current pipeline object is no longer valid.
    return self._pipe.ShareOutputs()
RuntimeError    : return self._pipe.ShareOutputs()Critical error in pipeline:
Error when executing GPU operator Rotate, instance name: "__Rotate_5", encountered:
CUDA allocation failed
Current pipeline object is no longer valid.

RuntimeError: Critical error in pipeline:
Error when executing GPU operator Rotate, instance name: "__Rotate_5", encountered:
CUDA allocation failed
Current pipeline object is no longer valid.
    train_loader = RandomCropDataLoader(params, num_workers=params.num_data_workers, device_id=device_id)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 306, in __init__
    self.iterator = DALIGenericIterator([self.pipe], ['inp', 'tar'], self.length, auto_reset = True)
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 160, in __init__
    self._first_batch = self.next()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py", line 273, in next
    return self.__next__()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 174, in __next__
    outputs.append(p.share_outputs())
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/pipeline.py", line 663, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error when executing GPU operator Rotate, instance name: "__Rotate_5", encountered:
CUDA allocation failed
Current pipeline object is no longer valid.
Traceback (most recent call last):
  File "train.py", line 193, in <module>
    train(params, args, world_rank)
  File "train.py", line 42, in train
    train_data_loader = get_data_loader_distributed(params, world_rank)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 23, in get_data_loader_distributed
    train_loader = RandomCropDataLoader(params, num_workers=params.num_data_workers, device_id=device_id)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 306, in __init__
    self.iterator = DALIGenericIterator([self.pipe], ['inp', 'tar'], self.length, auto_reset = True)
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 160, in __init__
    self._first_batch = self.next()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py", line 273, in next
    return self.__next__()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 174, in __next__
    outputs.append(p.share_outputs())
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/pipeline.py", line 663, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error when executing GPU operator Rotate, instance name: "__Rotate_5", encountered:
CUDA allocation failed
Current pipeline object is no longer valid.
Traceback (most recent call last):
  File "train.py", line 193, in <module>
    train(params, args, world_rank)
  File "train.py", line 42, in train
    train_data_loader = get_data_loader_distributed(params, world_rank)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 23, in get_data_loader_distributed
    train_loader = RandomCropDataLoader(params, num_workers=params.num_data_workers, device_id=device_id)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 306, in __init__
    self.iterator = DALIGenericIterator([self.pipe], ['inp', 'tar'], self.length, auto_reset = True)
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 160, in __init__
    self._first_batch = self.next()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py", line 273, in next
    return self.__next__()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 174, in __next__
    outputs.append(p.share_outputs())
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/pipeline.py", line 663, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error when executing GPU operator Rotate, instance name: "__Rotate_5", encountered:
CUDA allocation failed
Current pipeline object is no longer valid.
Traceback (most recent call last):
  File "train.py", line 193, in <module>
    train(params, args, world_rank)
  File "train.py", line 42, in train
    train_data_loader = get_data_loader_distributed(params, world_rank)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 23, in get_data_loader_distributed
    train_loader = RandomCropDataLoader(params, num_workers=params.num_data_workers, device_id=device_id)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 306, in __init__
    self.iterator = DALIGenericIterator([self.pipe], ['inp', 'tar'], self.length, auto_reset = True)
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 160, in __init__
    self._first_batch = self.next()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py", line 273, in next
    return self.__next__()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 174, in __next__
    outputs.append(p.share_outputs())
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/pipeline.py", line 663, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error when executing GPU operator Rotate, instance name: "__Rotate_5", encountered:
CUDA allocation failed
Current pipeline object is no longer valid.
Traceback (most recent call last):
  File "train.py", line 193, in <module>
    train(params, args, world_rank)
  File "train.py", line 42, in train
    train_data_loader = get_data_loader_distributed(params, world_rank)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 23, in get_data_loader_distributed
    train_loader = RandomCropDataLoader(params, num_workers=params.num_data_workers, device_id=device_id)
  File "/global/cfs/cdirs/dasrepo/tkurth/DataScience/cosmo-3D/src/DDP_UNet_original/utils/data_loader_dali_cupy_opt.py", line 306, in __init__
    self.iterator = DALIGenericIterator([self.pipe], ['inp', 'tar'], self.length, auto_reset = True)
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 160, in __init__
    self._first_batch = self.next()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/base_iterator.py", line 273, in next
    return self.__next__()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/plugin/pytorch.py", line 174, in __next__
    outputs.append(p.share_outputs())
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/nvidia/dali/pipeline.py", line 663, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error when executing GPU operator Rotate, instance name: "__Rotate_5", encountered:
CUDA allocation failed
Current pipeline object is no longer valid.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Traceback (most recent call last):
  File "/opt/python/cp37-cp37m/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/opt/python/cp37-cp37m/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in <module>
    main()
  File "/opt/python/cp37-cp37m/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/python/cp37-cp37m/bin/python', '-u', 'train.py', '--local_rank=7', '--run_num=14', '--yaml_config=./config/UNet_transpose.yaml', '--config=multi8']' returned non-zero exit status 1.
srun: error: cgpu07: task 0: Exited with exit code 1
srun: Terminating job step 838875.0
Wed Jul 22 13:00:52 PDT 2020
